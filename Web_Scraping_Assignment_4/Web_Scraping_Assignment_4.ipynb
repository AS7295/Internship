{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb95eb6",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a642fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries of selenium\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a04076",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "Scrape the details of most viewed videos on YouTube from Wikipedia. \n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos \n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54dc643a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>NAME</th>\n",
       "      <th>ARTISTS</th>\n",
       "      <th>VIEWS (in millions)</th>\n",
       "      <th>UPLOAD DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[6]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>12.85</td>\n",
       "      <td>June 17, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[9]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>8.16</td>\n",
       "      <td>January 12, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[16]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>6.70</td>\n",
       "      <td>October 8, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[17]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>6.20</td>\n",
       "      <td>May 2, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[18]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>6.00</td>\n",
       "      <td>January 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[21]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>5.89</td>\n",
       "      <td>April 6, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[26]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>5.30</td>\n",
       "      <td>March 6, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Wheels on the Bus\"[27]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>5.24</td>\n",
       "      <td>May 24, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[28]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>4.92</td>\n",
       "      <td>November 19, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[29]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>4.89</td>\n",
       "      <td>February 27, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[30]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>4.80</td>\n",
       "      <td>July 15, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[35]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.55</td>\n",
       "      <td>January 31, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[36]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>4.35</td>\n",
       "      <td>April 5, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[37]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>3.91</td>\n",
       "      <td>June 16, 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[38]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.87</td>\n",
       "      <td>January 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[39]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.80</td>\n",
       "      <td>September 5, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[40]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.79</td>\n",
       "      <td>May 31, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[41]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.66</td>\n",
       "      <td>October 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[42]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>3.64</td>\n",
       "      <td>June 25, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Thinking Out Loud\"[43]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.60</td>\n",
       "      <td>October 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[44]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>3.59</td>\n",
       "      <td>June 4, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Dark Horse\"[45]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.52</td>\n",
       "      <td>February 20, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[46]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>3.48</td>\n",
       "      <td>June 14, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Faded\"[47]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>3.45</td>\n",
       "      <td>December 3, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Perfect\"[48]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.45</td>\n",
       "      <td>November 9, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[49]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>3.44</td>\n",
       "      <td>July 25, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Girls Like You\"[50]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.42</td>\n",
       "      <td>May 31, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[51]</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>3.41</td>\n",
       "      <td>January 26, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Lean On\"[52]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>3.38</td>\n",
       "      <td>March 22, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Bailando\"[53]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>3.38</td>\n",
       "      <td>April 11, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                                             NAME  \\\n",
       "0    1.                            \"Baby Shark Dance\"[6]   \n",
       "1    2.                                   \"Despacito\"[9]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[16]   \n",
       "3    4.                                  \"Bath Song\"[17]   \n",
       "4    5.                               \"Shape of You\"[18]   \n",
       "5    6.                              \"See You Again\"[21]   \n",
       "6    7.                \"Phonics Song with Two Words\"[26]   \n",
       "7    8.                          \"Wheels on the Bus\"[27]   \n",
       "8    9.                                \"Uptown Funk\"[28]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[29]   \n",
       "10  11.                              \"Gangnam Style\"[30]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[35]   \n",
       "12  13.                             \"Dame Tu Cosita\"[36]   \n",
       "13  14.                                     \"Axel F\"[37]   \n",
       "14  15.                                      \"Sugar\"[38]   \n",
       "15  16.                                       \"Roar\"[39]   \n",
       "16  17.                             \"Counting Stars\"[40]   \n",
       "17  18.                                      \"Sorry\"[41]   \n",
       "18  19.                        \"Baa Baa Black Sheep\"[42]   \n",
       "19  20.                          \"Thinking Out Loud\"[43]   \n",
       "20  21.           \"Waka Waka (This Time for Africa)\"[44]   \n",
       "21  22.                                 \"Dark Horse\"[45]   \n",
       "22  23.                             \"Lakdi Ki Kathi\"[46]   \n",
       "23  24.                                      \"Faded\"[47]   \n",
       "24  25.                                    \"Perfect\"[48]   \n",
       "25  26.                                 \"Let Her Go\"[49]   \n",
       "26  27.                             \"Girls Like You\"[50]   \n",
       "27  28.          \"Humpty the train on a fruits ride\"[51]   \n",
       "28  29.                                    \"Lean On\"[52]   \n",
       "29  30.                                   \"Bailando\"[53]   \n",
       "\n",
       "                                          ARTISTS VIEWS (in millions)  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories               12.85   \n",
       "1                                      Luis Fonsi                8.16   \n",
       "2                                     LooLoo Kids                6.70   \n",
       "3                      Cocomelon – Nursery Rhymes                6.20   \n",
       "4                                      Ed Sheeran                6.00   \n",
       "5                                     Wiz Khalifa                5.89   \n",
       "6                                       ChuChu TV                5.30   \n",
       "7                      Cocomelon – Nursery Rhymes                5.24   \n",
       "8                                     Mark Ronson                4.92   \n",
       "9                                     Miroshka TV                4.89   \n",
       "10                                            Psy                4.80   \n",
       "11                                     Get Movies                4.55   \n",
       "12                                      El Chombo                4.35   \n",
       "13                                     Crazy Frog                3.91   \n",
       "14                                       Maroon 5                3.87   \n",
       "15                                     Katy Perry                3.80   \n",
       "16                                    OneRepublic                3.79   \n",
       "17                                  Justin Bieber                3.66   \n",
       "18                     Cocomelon – Nursery Rhymes                3.64   \n",
       "19                                     Ed Sheeran                3.60   \n",
       "20                                        Shakira                3.59   \n",
       "21                                     Katy Perry                3.52   \n",
       "22                                   Jingle Toons                3.48   \n",
       "23                                    Alan Walker                3.45   \n",
       "24                                     Ed Sheeran                3.45   \n",
       "25                                      Passenger                3.44   \n",
       "26                                       Maroon 5                3.42   \n",
       "27  Kiddiestv Hindi – Nursery Rhymes & Kids Songs                3.41   \n",
       "28                                    Major Lazer                3.38   \n",
       "29                               Enrique Iglesias                3.38   \n",
       "\n",
       "          UPLOAD DATE  \n",
       "0       June 17, 2016  \n",
       "1    January 12, 2017  \n",
       "2     October 8, 2016  \n",
       "3         May 2, 2018  \n",
       "4    January 30, 2017  \n",
       "5       April 6, 2015  \n",
       "6       March 6, 2014  \n",
       "7        May 24, 2018  \n",
       "8   November 19, 2014  \n",
       "9   February 27, 2018  \n",
       "10      July 15, 2012  \n",
       "11   January 31, 2012  \n",
       "12      April 5, 2018  \n",
       "13      June 16, 2009  \n",
       "14   January 14, 2015  \n",
       "15  September 5, 2013  \n",
       "16       May 31, 2013  \n",
       "17   October 22, 2015  \n",
       "18      June 25, 2018  \n",
       "19    October 7, 2014  \n",
       "20       June 4, 2010  \n",
       "21  February 20, 2014  \n",
       "22      June 14, 2018  \n",
       "23   December 3, 2015  \n",
       "24   November 9, 2017  \n",
       "25      July 25, 2012  \n",
       "26       May 31, 2018  \n",
       "27   January 26, 2018  \n",
       "28     March 22, 2015  \n",
       "29     April 11, 2014  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the driver and getting the url\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "url='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)\n",
    "sleep(10)\n",
    "\n",
    "#getting each data row wise from the table like structure data. \n",
    "ranks=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[1]')[0:30]\n",
    "names=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[2]')[0:30]\n",
    "artists=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[3]')[0:30]\n",
    "views=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[4]')[0:30]\n",
    "upload_date=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[5]')[0:30]\n",
    "\n",
    "#creating empty list\n",
    "most_views_youtube=[]\n",
    "#creating empty dictionary for temporary usage\n",
    "table={}\n",
    "\n",
    "# iterate each lists to get into dataframe \n",
    "for i in range(len(ranks)):\n",
    "    table={\"RANK\":ranks[i].text,\n",
    "           \" NAME\":names[i].text,\n",
    "           \"ARTISTS\":artists[i].text,\n",
    "           \"VIEWS (in millions)\": views[i].text,\n",
    "           \"UPLOAD DATE\" : upload_date[i].text}\n",
    "    most_views_youtube.append(table)\n",
    "\n",
    "#quit the driver\n",
    "driver.quit()\n",
    "\n",
    "# making the dataframe\n",
    "df_data=pd.DataFrame(most_views_youtube)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc67d2",
   "metadata": {},
   "source": [
    "# Q2. \n",
    "Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ba9c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st T20I -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>9 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2nd T20I -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>11 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1st Test -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Windsor Park, Dominica</td>\n",
       "      <td>12 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd T20I -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>13 JUL 2023</td>\n",
       "      <td>1:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1st ODI -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>16 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2nd ODI -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>19 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2nd Test -</td>\n",
       "      <td>INDIA TOUR OF WEST INDIES 2023</td>\n",
       "      <td>Queen's Park Oval, Trinidad</td>\n",
       "      <td>20 JUL 2023</td>\n",
       "      <td>7:30 PM IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3rd ODI -</td>\n",
       "      <td>INDIA WOMEN TOUR OF BANGLADESH 2023</td>\n",
       "      <td>Shere Bangla National Stadium, Mirpur, Dhaka</td>\n",
       "      <td>22 JUL 2023</td>\n",
       "      <td>9:00 AM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                               Series  \\\n",
       "0  1st T20I -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "1  2nd T20I -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "2  1st Test -       INDIA TOUR OF WEST INDIES 2023   \n",
       "3  3rd T20I -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "4   1st ODI -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "5   2nd ODI -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "6  2nd Test -       INDIA TOUR OF WEST INDIES 2023   \n",
       "7   3rd ODI -  INDIA WOMEN TOUR OF BANGLADESH 2023   \n",
       "\n",
       "                                           Place         Date         Time  \n",
       "0   Shere Bangla National Stadium, Mirpur, Dhaka   9 JUL 2023  1:30 PM IST  \n",
       "1   Shere Bangla National Stadium, Mirpur, Dhaka  11 JUL 2023  1:30 PM IST  \n",
       "2                         Windsor Park, Dominica  12 JUL 2023  7:30 PM IST  \n",
       "3   Shere Bangla National Stadium, Mirpur, Dhaka  13 JUL 2023  1:30 PM IST  \n",
       "4   Shere Bangla National Stadium, Mirpur, Dhaka  16 JUL 2023  9:00 AM IST  \n",
       "5   Shere Bangla National Stadium, Mirpur, Dhaka  19 JUL 2023  9:00 AM IST  \n",
       "6                    Queen's Park Oval, Trinidad  20 JUL 2023  7:30 PM IST  \n",
       "7   Shere Bangla National Stadium, Mirpur, Dhaka  22 JUL 2023  9:00 AM IST  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the driver  \n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "#loading the url into the driver\n",
    "url='https://www.bcci.tv/'\n",
    "driver.get(url)\n",
    "# webdriverwait to wait the driver explicitely \n",
    "WebDriverWait(driver,20).until(EC.presence_of_element_located((By.CLASS_NAME,\"footer-bottom \")))\n",
    "\n",
    "#click on the international fixtures\n",
    "international=driver.find_element(By.XPATH,'//li[@class=\"nav-item\"]/a')\n",
    "international.click()\n",
    "sleep(10)\n",
    "\n",
    "# getting each element in a listed form\n",
    "Match_title=driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]')\n",
    "tour=driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "place=driver.find_elements(By.XPATH,'//div[@class=\"match-place ng-scope\"]')\n",
    "date=driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "time=driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "\n",
    "# creating empty list\n",
    "fixture=[]\n",
    "# creating empty dictionary\n",
    "temporary_dict={}\n",
    "\n",
    "# iterate the scraped list to get into listed dictionary(key value pair)\n",
    "for i in range(len(Match_title)):\n",
    "    temporary_dict={\"Match Title\": Match_title[i].text,\n",
    "                    \"Series\":tour[i].text, \n",
    "                    \"Place\":place[i].text.split(\"-\")[1], \n",
    "                    \"Date\" : date[i].text,\n",
    "                    \"Time\":time[i].text}\n",
    "    fixture.append(temporary_dict)\n",
    "#quit the driver\n",
    "driver.quit()\n",
    "\n",
    "# creating the listed dictionary into dataframe format\n",
    "df_fixtures=pd.DataFrame(fixture)\n",
    "df_fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaec046",
   "metadata": {},
   "source": [
    "# Q3. \n",
    "Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98499c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the driver  \n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "#loading the url\n",
    "url='https://www.statisticstimes.com/'\n",
    "driver.get(url)\n",
    "sleep(5)\n",
    "\n",
    "#got it button\n",
    "try:\n",
    "    got_it=driver.find_element(By.XPATH,'/html/body/div[1]/div/a')\n",
    "    got_it.click()\n",
    "except:\n",
    "    print(\"exception handled fot got it\")\n",
    "\n",
    "#click on economy\n",
    "economy=driver.find_elements(By.XPATH,'//div[@class=\"navbar\"]/div/button')\n",
    "economy[1].click()\n",
    "\n",
    "# click on india\n",
    "ind=driver.find_element(By.XPATH,'//div[@class=\"navbar\"]/div[2]/div/a[3]')\n",
    "ind.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d44a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>STATE</th>\n",
       "      <th>GSDP (Cr INR at Current prices) 18-19</th>\n",
       "      <th>GSDP (Cr INR at Current prices) 19-20</th>\n",
       "      <th>SHARE 18-19</th>\n",
       "      <th>GDP ($billion) 2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                      STATE GSDP (Cr INR at Current prices) 18-19  \\\n",
       "0     1                Maharashtra                             2,632,792   \n",
       "1     2                 Tamil Nadu                             1,630,208   \n",
       "2     3              Uttar Pradesh                             1,584,764   \n",
       "3     4                    Gujarat                             1,502,899   \n",
       "4     5                  Karnataka                             1,493,127   \n",
       "5     6                West Bengal                             1,089,898   \n",
       "6     7                  Rajasthan                               942,586   \n",
       "7     8             Andhra Pradesh                               862,957   \n",
       "8     9                  Telangana                               861,031   \n",
       "9    10             Madhya Pradesh                               809,592   \n",
       "10   11                     Kerala                               781,653   \n",
       "11   12                      Delhi                               774,870   \n",
       "12   13                    Haryana                               734,163   \n",
       "13   14                      Bihar                               530,363   \n",
       "14   15                     Punjab                               526,376   \n",
       "15   16                     Odisha                               487,805   \n",
       "16   17                      Assam                               315,881   \n",
       "17   18               Chhattisgarh                               304,063   \n",
       "18   19                  Jharkhand                               297,204   \n",
       "19   20                Uttarakhand                               245,895   \n",
       "20   21            Jammu & Kashmir                               155,956   \n",
       "21   22           Himachal Pradesh                               153,845   \n",
       "22   23                        Goa                                73,170   \n",
       "23   24                    Tripura                                49,845   \n",
       "24   25                 Chandigarh                                42,114   \n",
       "25   26                 Puducherry                                34,433   \n",
       "26   27                  Meghalaya                                33,481   \n",
       "27   28                     Sikkim                                28,723   \n",
       "28   29                    Manipur                                27,870   \n",
       "29   30                   Nagaland                                27,283   \n",
       "30   31          Arunachal Pradesh                                24,603   \n",
       "31   32                    Mizoram                                22,287   \n",
       "32   33  Andaman & Nicobar Islands                                     -   \n",
       "\n",
       "   GSDP (Cr INR at Current prices) 19-20 SHARE 18-19 GDP ($billion) 2019  \n",
       "0                                      -      13.94%             399.921  \n",
       "1                              1,845,853       8.63%             247.629  \n",
       "2                              1,687,818       8.39%             240.726  \n",
       "3                                      -       7.96%             228.290  \n",
       "4                              1,631,977       7.91%             226.806  \n",
       "5                              1,253,832       5.77%             165.556  \n",
       "6                              1,020,989       4.99%             143.179  \n",
       "7                                972,782       4.57%             131.083  \n",
       "8                                969,604       4.56%             130.791  \n",
       "9                                906,672       4.29%             122.977  \n",
       "10                                     -       4.14%             118.733  \n",
       "11                               856,112       4.10%             117.703  \n",
       "12                               831,610       3.89%             111.519  \n",
       "13                               611,804       2.81%              80.562  \n",
       "14                               574,760       2.79%              79.957  \n",
       "15                               521,275       2.58%              74.098  \n",
       "16                                     -       1.67%              47.982  \n",
       "17                               329,180       1.61%              46.187  \n",
       "18                               328,598       1.57%              45.145  \n",
       "19                                     -       1.30%              37.351  \n",
       "20                                     -       0.83%              23.690  \n",
       "21                               165,472       0.81%              23.369  \n",
       "22                                80,449       0.39%              11.115  \n",
       "23                                55,984       0.26%               7.571  \n",
       "24                                     -       0.22%               6.397  \n",
       "25                                38,253       0.18%               5.230  \n",
       "26                                36,572       0.18%               5.086  \n",
       "27                                32,496       0.15%               4.363  \n",
       "28                                31,790       0.15%               4.233  \n",
       "29                                     -       0.14%               4.144  \n",
       "30                                     -       0.13%               3.737  \n",
       "31                                26,503       0.12%               3.385  \n",
       "32                                     -           -                   -  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#click on indian state\n",
    "indian_states=driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "indian_states.click()\n",
    "sleep(10)\n",
    "\n",
    "# getting each data from table into listed form\n",
    "rank=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "state=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "gsdp_18_19=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "gsdp_19_20=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "share_18_19=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "gdp=driver.find_elements(By.XPATH,'//table[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "\n",
    "#create empty list \n",
    "data_list=[]\n",
    "#create temporary dictionary\n",
    "temp_dict={}\n",
    "\n",
    "#iterate the listed data to form listed dictionary\n",
    "for i in range(len(rank)):\n",
    "    temp_dict={\"RANK\":rank[i].text,\n",
    "              \"STATE\":state[i].text,\n",
    "              \"GSDP (Cr INR at Current prices) 18-19\" : gsdp_18_19[i].text,\n",
    "              \"GSDP (Cr INR at Current prices) 19-20\" :gsdp_19_20[i].text,\n",
    "              \"SHARE 18-19\" : share_18_19[i].text,\n",
    "              \"GDP ($billion) 2019\" : gdp[i].text}\n",
    "    data_list.append(temp_dict)\n",
    "#quit the driver\n",
    "driver.quit()\n",
    "\n",
    "#creating dataframe\n",
    "df_economy=pd.DataFrame(data_list)\n",
    "df_economy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856fc19",
   "metadata": {},
   "source": [
    "# Q4. \n",
    "Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used.\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6038e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributor Count</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DragGAN</td>\n",
       "      <td>Official Code for DragGAN (SIGGRAPH 2023)</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ChatGLM2-6B</td>\n",
       "      <td>ChatGLM2-6B: An Open Bilingual Chat LLM | 开源双语...</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FastSAM</td>\n",
       "      <td>Fast Segment Anything</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>freegpt-webui</td>\n",
       "      <td>GPT 3.5/4 with a Chat Web UI. No API key requi...</td>\n",
       "      <td>3</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embedchain</td>\n",
       "      <td>Framework to easily create LLM powered bots ov...</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spacedrive</td>\n",
       "      <td>Spacedrive is an open source cross-platform fi...</td>\n",
       "      <td>64</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open-resume</td>\n",
       "      <td>OpenResume is a powerful open-source resume bu...</td>\n",
       "      <td></td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>papers-we-love</td>\n",
       "      <td>Papers from the computer science community to ...</td>\n",
       "      <td></td>\n",
       "      <td>Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>skateshop</td>\n",
       "      <td>An open source e-commerce skateshop build with...</td>\n",
       "      <td></td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Web-Dev-For-Beginners</td>\n",
       "      <td>24 Lessons, 12 Weeks, Get Started as a Web Dev...</td>\n",
       "      <td></td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>diy-spacemouse</td>\n",
       "      <td>A DIY navigation device for Fusion360</td>\n",
       "      <td></td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ChatGLM-6B</td>\n",
       "      <td>ChatGLM-6B: An Open Bilingual Dialogue Languag...</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PanoHead</td>\n",
       "      <td>Code Repository for CVPR 2023 Paper \"PanoHead:...</td>\n",
       "      <td></td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>awesome-chatgpt-prompts-zh</td>\n",
       "      <td>ChatGPT 中文调教指南。各种场景使用指南。学习怎么让它听你的话。</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>first-contributions</td>\n",
       "      <td>🚀✨ Help beginners to contribute to open source...</td>\n",
       "      <td>5,000+</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>actual</td>\n",
       "      <td>A local-first personal finance system</td>\n",
       "      <td></td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpt4free</td>\n",
       "      <td>The official gpt4free repository | various col...</td>\n",
       "      <td>83</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>svelte</td>\n",
       "      <td>Cybernetically enhanced web apps</td>\n",
       "      <td></td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DragGAN</td>\n",
       "      <td>Unofficial Implementation of DragGAN - \"Drag Y...</td>\n",
       "      <td>9</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UniAD</td>\n",
       "      <td>[CVPR 2023 Best Paper] Planning-oriented Auton...</td>\n",
       "      <td>6</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>QGIS</td>\n",
       "      <td>QGIS is a free, open source, cross platform (l...</td>\n",
       "      <td>20</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chat2DB</td>\n",
       "      <td>🔥 🔥 🔥 An intelligent and versatile general-pur...</td>\n",
       "      <td></td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pygwalker</td>\n",
       "      <td>PyGWalker: Turn your pandas dataframe into a T...</td>\n",
       "      <td>11</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ggml</td>\n",
       "      <td>Tensor library for machine learning</td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>quivr</td>\n",
       "      <td>🧠 Dump all your files and thoughts into your p...</td>\n",
       "      <td></td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Repository Title  \\\n",
       "0                      DragGAN   \n",
       "1                  ChatGLM2-6B   \n",
       "2                      FastSAM   \n",
       "3                freegpt-webui   \n",
       "4                   embedchain   \n",
       "5                   spacedrive   \n",
       "6                  open-resume   \n",
       "7               papers-we-love   \n",
       "8                    skateshop   \n",
       "9        Web-Dev-For-Beginners   \n",
       "10              diy-spacemouse   \n",
       "11                  ChatGLM-6B   \n",
       "12                    PanoHead   \n",
       "13  awesome-chatgpt-prompts-zh   \n",
       "14         first-contributions   \n",
       "15                      actual   \n",
       "16                    gpt4free   \n",
       "17                      svelte   \n",
       "18                     DragGAN   \n",
       "19                       UniAD   \n",
       "20                        QGIS   \n",
       "21                     Chat2DB   \n",
       "22                   pygwalker   \n",
       "23                        ggml   \n",
       "24                       quivr   \n",
       "\n",
       "                               Repository Description Contributor Count  \\\n",
       "0           Official Code for DragGAN (SIGGRAPH 2023)                     \n",
       "1   ChatGLM2-6B: An Open Bilingual Chat LLM | 开源双语...                     \n",
       "2                               Fast Segment Anything                     \n",
       "3   GPT 3.5/4 with a Chat Web UI. No API key requi...                 3   \n",
       "4   Framework to easily create LLM powered bots ov...                     \n",
       "5   Spacedrive is an open source cross-platform fi...                64   \n",
       "6   OpenResume is a powerful open-source resume bu...                     \n",
       "7   Papers from the computer science community to ...                     \n",
       "8   An open source e-commerce skateshop build with...                     \n",
       "9   24 Lessons, 12 Weeks, Get Started as a Web Dev...                     \n",
       "10              A DIY navigation device for Fusion360                     \n",
       "11  ChatGLM-6B: An Open Bilingual Dialogue Languag...                     \n",
       "12  Code Repository for CVPR 2023 Paper \"PanoHead:...                     \n",
       "13                ChatGPT 中文调教指南。各种场景使用指南。学习怎么让它听你的话。                     \n",
       "14  🚀✨ Help beginners to contribute to open source...            5,000+   \n",
       "15              A local-first personal finance system                     \n",
       "16  The official gpt4free repository | various col...                83   \n",
       "17                   Cybernetically enhanced web apps                     \n",
       "18  Unofficial Implementation of DragGAN - \"Drag Y...                 9   \n",
       "19  [CVPR 2023 Best Paper] Planning-oriented Auton...                 6   \n",
       "20  QGIS is a free, open source, cross platform (l...                20   \n",
       "21  🔥 🔥 🔥 An intelligent and versatile general-pur...                     \n",
       "22  PyGWalker: Turn your pandas dataframe into a T...                11   \n",
       "23                Tensor library for machine learning                     \n",
       "24  🧠 Dump all your files and thoughts into your p...                     \n",
       "\n",
       "      Language  \n",
       "0       Python  \n",
       "1       Python  \n",
       "2       Python  \n",
       "3       Python  \n",
       "4       Python  \n",
       "5         Rust  \n",
       "6   TypeScript  \n",
       "7        Shell  \n",
       "8   TypeScript  \n",
       "9   JavaScript  \n",
       "10         C++  \n",
       "11      Python  \n",
       "12      Python  \n",
       "13        None  \n",
       "14        None  \n",
       "15  JavaScript  \n",
       "16      Python  \n",
       "17  JavaScript  \n",
       "18      Python  \n",
       "19      Python  \n",
       "20         C++  \n",
       "21        Java  \n",
       "22      Python  \n",
       "23           C  \n",
       "24  TypeScript  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the driver  \n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "#loading the url\n",
    "url='https://github.com/'\n",
    "driver.get(url)\n",
    "sleep(5)\n",
    "\n",
    "# Click on open source\n",
    "open_source=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/button')\n",
    "open_source.click()\n",
    "sleep(5)\n",
    "\n",
    "#click on trending\n",
    "trending=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "trending.click()\n",
    "sleep(5)\n",
    "\n",
    "#getting url of each tending data available on github\n",
    "url=[]\n",
    "urls=driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/h2/a')\n",
    "for i in urls:\n",
    "    url.append(i.get_attribute(\"href\"))\n",
    "print(len(url))\n",
    "\n",
    "\n",
    "#creating empty list\n",
    "title=[]\n",
    "description=[]\n",
    "contributors=[]\n",
    "language=[]\n",
    "\n",
    "# iterate each url \n",
    "for i in range(len(url)):\n",
    "    driver.get(url[i])\n",
    "    sleep(5)\n",
    "    \n",
    "    #getting title \n",
    "    try:\n",
    "        title_=driver.find_element(By.XPATH,'//div[@class=\"flex-auto min-width-0 width-fit mr-3\"]/div/strong')\n",
    "        title.append(title_.text)\n",
    "    except NoSuchElementException:\n",
    "        title.append(None)\n",
    "    \n",
    "    #Getting description\n",
    "    try:\n",
    "        desc=driver.find_element(By.XPATH,'//div[@class=\"BorderGrid-cell\"]/p')\n",
    "        description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        description.append(None)\n",
    "    \n",
    "    try:\n",
    "        contributor=driver.find_element(By.XPATH,'//span[@class=\"Counter ml-1\"]')\n",
    "        contributors.append(contributor.text)\n",
    "    except NoSuchElementException:\n",
    "        contributors.append(None)\n",
    "    \n",
    "    try:\n",
    "        lang=driver.find_element(By.XPATH,'//span[@class=\"color-fg-default text-bold mr-1\"]')\n",
    "        language.append(lang.text)\n",
    "    except NoSuchElementException:\n",
    "        language.append(None)\n",
    "    \n",
    "#quit the driver\n",
    "driver.quit()\n",
    "\n",
    "df_github=pd.DataFrame({\"Repository Title\":title,\n",
    "                 \"Repository Description\":description,\n",
    "                 \"Contributor Count\":contributors,\n",
    "                 \"Language\":language})\n",
    "df_github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c30186",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https:/www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb37546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This Week Rank</th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks On Chart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fast Car</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Calm Down</td>\n",
       "      <td>Rema &amp; Selena Gomez</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Angel, Pt. 1</td>\n",
       "      <td>Kodak Black, NLE Choppa, Jimin, JVKE &amp; Muni Long</td>\n",
       "      <td>-</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Girl In Mine</td>\n",
       "      <td>Parmalee</td>\n",
       "      <td>-</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Moonlight</td>\n",
       "      <td>Kali Uchis</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Classy 101</td>\n",
       "      <td>Feid x Young Miko</td>\n",
       "      <td>-</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Bluffin</td>\n",
       "      <td>Gucci Mane &amp; Lil Baby</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   This Week Rank     Song Name  \\\n",
       "0               1    Last Night   \n",
       "1               2      Fast Car   \n",
       "2               3     Calm Down   \n",
       "3               4       Flowers   \n",
       "4               5   All My Life   \n",
       "..            ...           ...   \n",
       "95             96  Angel, Pt. 1   \n",
       "96             97  Girl In Mine   \n",
       "97             98     Moonlight   \n",
       "98             99    Classy 101   \n",
       "99            100       Bluffin   \n",
       "\n",
       "                                         Artist Name Last Week Rank Peak Rank  \\\n",
       "0                                      Morgan Wallen              1         1   \n",
       "1                                         Luke Combs              3         2   \n",
       "2                                Rema & Selena Gomez              4         3   \n",
       "3                                        Miley Cyrus              2         1   \n",
       "4                         Lil Durk Featuring J. Cole              5         2   \n",
       "..                                               ...            ...       ...   \n",
       "95  Kodak Black, NLE Choppa, Jimin, JVKE & Muni Long              -        65   \n",
       "96                                          Parmalee              -        97   \n",
       "97                                        Kali Uchis             90        80   \n",
       "98                                 Feid x Young Miko              -        99   \n",
       "99                             Gucci Mane & Lil Baby              -       100   \n",
       "\n",
       "   Weeks On Chart  \n",
       "0              21  \n",
       "1              13  \n",
       "2              42  \n",
       "3              23  \n",
       "4               6  \n",
       "..            ...  \n",
       "95              2  \n",
       "96              1  \n",
       "97             11  \n",
       "98              1  \n",
       "99              1  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "#getting url and load into driver\n",
    "url='https:/www.billboard.com/'\n",
    "driver.get(url)\n",
    "sleep(5)\n",
    "\n",
    "#click on chart\n",
    "charts=driver.find_element(By.XPATH,'/html/body/div[3]/header/div/div[2]/div/div/div[2]/div[2]/div/div/nav/ul/li[1]/a')\n",
    "charts.click()\n",
    "sleep(5)\n",
    "\n",
    "# click on hot 100\n",
    "hot_100=driver.find_element(By.XPATH,'/html/body/div[3]/main/div[2]/div[1]/div[1]/div/div/div[1]/div/div[2]/span/a')\n",
    "hot_100.click()\n",
    "sleep(5)\n",
    "\n",
    "#click on close\n",
    "close=driver.find_element(By.XPATH,'/html/body/div[2]/div/span')\n",
    "close.click()\n",
    "\n",
    "#getting all data into listed form\n",
    "this_week=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[1]')\n",
    "song=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li/h3')\n",
    "artist=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[1]/span')\n",
    "last_week_rank=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[4]/span')\n",
    "peak_rank=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[5]/span')\n",
    "weeks=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li[6]/span')\n",
    "\n",
    "#creating empty list\n",
    "hot_100_songs=[]\n",
    "#creating empty dictionary for temporary purpose\n",
    "temp_dict={}\n",
    "\n",
    "# iterate all listed data to create listed dictionary\n",
    "for i in range(len(this_week)):\n",
    "    temp_dict={\"This Week Rank\":this_week[i].text,\n",
    "               \"Song Name\": song[i].text,\n",
    "               \"Artist Name\": artist[i].text,\n",
    "               \"Last Week Rank\": last_week_rank[i].text ,\n",
    "               \"Peak Rank\" : peak_rank[i].text,\n",
    "               \"Weeks On Chart\": weeks[i].text}\n",
    "    hot_100_songs.append(temp_dict)\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()\n",
    "\n",
    "# create DataFrame    \n",
    "df_hot_100_songs=pd.DataFrame(hot_100_songs)\n",
    "df_hot_100_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33984fd",
   "metadata": {},
   "source": [
    "# Q6. \n",
    "Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3472653a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>VOLUMES SOLD</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>GENERE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                                              TITLE            AUTHOR  \\\n",
       "0     1                                  Da Vinci Code,The        Brown, Dan   \n",
       "1     2               Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2     3           Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3     4          Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4     5                               Fifty Shades of Grey      James, E. L.   \n",
       "..  ...                                                ...               ...   \n",
       "95   96                                          Ghost,The    Harris, Robert   \n",
       "96   97                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97   98              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98   99  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   VOLUMES SOLD        PUBLISHER                       GENERE  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "#loading the url into driver\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "driver.get(url)\n",
    "sleep(10)\n",
    "\n",
    "# getting all data from the table into listed form\n",
    "rank=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[1]')\n",
    "title=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[2]')\n",
    "author=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[3]')\n",
    "volume_sales=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[4]')\n",
    "publisher=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[5]')\n",
    "genere=driver.find_elements(By.XPATH,'//table[@class=\"in-article sortable\"]/tbody/tr/td[6]')\n",
    "\n",
    "#creating empty list\n",
    "novels=[]\n",
    "# creating empty dictionary for temporary purpose\n",
    "temp_dict={}\n",
    "\n",
    "# Iterate listed data to form listed dictionary\n",
    "for i in range(len(rank)):\n",
    "    temp_dict={\"RANK\": rank[i].text,\n",
    "               \"TITLE\": title[i].text,\n",
    "               \"AUTHOR\" : author[i].text,\n",
    "               \"VOLUMES SOLD\":volume_sales[i].text,\n",
    "               \"PUBLISHER\": publisher[i].text,\n",
    "               \"GENERE\":genere[i].text}\n",
    "    novels.append(temp_dict)\n",
    "    \n",
    "# Quit the driver\n",
    "driver.quit()\n",
    "    \n",
    "# making DataFrame\n",
    "novel_df=pd.DataFrame(novels)\n",
    "novel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe886a",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53ff33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>NAME</th>\n",
       "      <th>YEAR OF SPAN</th>\n",
       "      <th>GENERE</th>\n",
       "      <th>RUN TIME</th>\n",
       "      <th>RATINGS</th>\n",
       "      <th>VOTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,173,715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,251,542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,032,493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>303,560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>262,731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>51,957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>63,993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>208,546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>43,402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>260,203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RANK                            NAME YEAR OF SPAN  \\\n",
       "0     1.                 Game of Thrones  (2011–2019)   \n",
       "1     2.                 Stranger Things  (2016–2024)   \n",
       "2     3.                The Walking Dead  (2010–2022)   \n",
       "3     4.                  13 Reasons Why  (2017–2020)   \n",
       "4     5.                         The 100  (2014–2020)   \n",
       "..   ...                             ...          ...   \n",
       "95   96.                           Reign  (2013–2017)   \n",
       "96   97.  A Series of Unfortunate Events  (2017–2019)   \n",
       "97   98.                  Criminal Minds     (2005– )   \n",
       "98   99.           Scream: The TV Series  (2015–2019)   \n",
       "99  100.      The Haunting of Hill House       (2018)   \n",
       "\n",
       "                      GENERE RUN TIME RATINGS      VOTES  \n",
       "0   Action, Adventure, Drama   57 min     9.2  2,173,715  \n",
       "1     Drama, Fantasy, Horror   51 min     8.7  1,251,542  \n",
       "2    Drama, Horror, Thriller   44 min     8.1  1,032,493  \n",
       "3   Drama, Mystery, Thriller   60 min     7.5    303,560  \n",
       "4     Drama, Mystery, Sci-Fi   43 min     7.6    262,731  \n",
       "..                       ...      ...     ...        ...  \n",
       "95                     Drama   42 min     7.4     51,957  \n",
       "96  Adventure, Comedy, Drama   50 min     7.8     63,993  \n",
       "97     Crime, Drama, Mystery   42 min     8.1    208,546  \n",
       "98      Comedy, Crime, Drama   45 min     7.1     43,402  \n",
       "99    Drama, Horror, Mystery  572 min     8.6    260,203  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "\n",
    "# Loading the url page\n",
    "url='https://www.imdb.com/list/ls095964455/'\n",
    "driver.get(url)\n",
    "sleep(10)\n",
    "\n",
    "# getting all necessary data into listed form\n",
    "rank=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div/h3/span[1]')\n",
    "name=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div/h3/a')\n",
    "year_span=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div/h3/span[2]')\n",
    "genere=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div/p/span[5]')\n",
    "run_time=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div/p/span[3]')\n",
    "ratings=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div[2]/div/div[1]/span[2]')\n",
    "votes=driver.find_elements(By.XPATH,'//div[@class=\"lister-item mode-detail\"]/div[2]/p[4]/span[2]')\n",
    "\n",
    "# creating empty list\n",
    "tv_series=[]\n",
    "# Creating empty dictionary for temporary purpose\n",
    "temp_disc={}\n",
    "\n",
    "# Iterate all the listed data to form listed dictionary\n",
    "for i in range(len(rank)):\n",
    "    temp_disc={\"RANK\":rank[i].text,\n",
    "               \"NAME\":name[i].text,\n",
    "               \"YEAR OF SPAN\": year_span[i].text,\n",
    "               \"GENERE\":genere[i].text,\n",
    "               \"RUN TIME\":run_time[i].text,\n",
    "               \"RATINGS\":ratings[i].text,\n",
    "               \"VOTES\":votes[i].text}\n",
    "    tv_series.append(temp_disc)\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()\n",
    "    \n",
    "#making dataframe:\n",
    "df_tv_series=pd.DataFrame(tv_series)\n",
    "df_tv_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af113e2d",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9c03757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Associated Tasks</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instances</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>150</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>303</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>13611</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>-</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>-</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>PMU-UD</td>\n",
       "      <td>Univariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>-</td>\n",
       "      <td>5180</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>Undocumented</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>EBL Domain Theories</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Moral Reasoner</td>\n",
       "      <td>Domain-Theory</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>202</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>DGP2 - The Second Data Generation Program</td>\n",
       "      <td>Data-Generator</td>\n",
       "      <td>-</td>\n",
       "      <td>Real</td>\n",
       "      <td>-</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Dataset Name                  Data Type  \\\n",
       "0                                         Iris               Multivariate   \n",
       "1                                Heart Disease               Multivariate   \n",
       "2                                        Adult               Multivariate   \n",
       "3                             Dry Bean Dataset               Multivariate   \n",
       "4                                     Diabetes  Multivariate, Time-Series   \n",
       "..                                         ...                        ...   \n",
       "618                                     PMU-UD                 Univariate   \n",
       "619                               Undocumented                          -   \n",
       "620                        EBL Domain Theories                          -   \n",
       "621                             Moral Reasoner              Domain-Theory   \n",
       "622  DGP2 - The Second Data Generation Program             Data-Generator   \n",
       "\n",
       "    Associated Tasks              Attribute Type No of Instances  Year  \n",
       "0     Classification                        Real             150  1988  \n",
       "1     Classification  Categorical, Integer, Real             303  1988  \n",
       "2     Classification        Categorical, Integer           48842  1996  \n",
       "3     Classification               Integer, Real           13611  2020  \n",
       "4                  -        Categorical, Integer               -  None  \n",
       "..               ...                         ...             ...   ...  \n",
       "618   Classification                           -            5180  2018  \n",
       "619                -                           -               -  None  \n",
       "620                -                           -               -  None  \n",
       "621                -                           -             202  1994  \n",
       "622                -                        Real               -  None  \n",
       "\n",
       "[623 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Loading the url into the driver\n",
    "url='https://archive.ics.uci.edu/'\n",
    "driver.get(url)\n",
    "sleep(5)\n",
    "\n",
    "# accept button\n",
    "accept_button=driver.find_element(By.XPATH,'//button[@class=\"btn-primary btn-sm btn m-1\"]')\n",
    "accept_button.click()\n",
    "\n",
    "# click on datasets\n",
    "datasets=driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a')\n",
    "datasets.click()\n",
    "sleep(5)\n",
    "\n",
    "# Getting URL of each dataset\n",
    "url=[]\n",
    "for i in range(63):\n",
    "    urls=driver.find_elements(By.XPATH,'//div[@class=\"relative col-span-8 sm:col-span-7\"]/h2/a')\n",
    "    for u in urls:\n",
    "        url.append(u.get_attribute(\"href\"))\n",
    "        \n",
    "    #click on next button\n",
    "    if len(url)<=620:\n",
    "        next_button=driver.find_element(By.XPATH,'//div[@class=\"btn-group\"]/button[2]')\n",
    "        next_button.click()\n",
    "        sleep(5)\n",
    "print(len(url))\n",
    "\n",
    "# creating empty list:\n",
    "name=[]\n",
    "data_type=[]\n",
    "task=[]\n",
    "attribute_type=[]\n",
    "no_of_instance=[]\n",
    "year=[]\n",
    "\n",
    "# Iterate each url and get necessary data\n",
    "for u in url:\n",
    "    driver.get(u)\n",
    "    sleep(5)\n",
    "    \n",
    "    # Dataset Name\n",
    "    try:\n",
    "        nm=driver.find_element(By.XPATH,'//h1[@class=\"text-3xl font-semibold text-primary-content\"]')\n",
    "        name.append(nm.text)\n",
    "    except NoSuchElementException:\n",
    "        name.append(None)\n",
    "    \n",
    "    # Data type\n",
    "    try:\n",
    "        d_type=driver.find_element(By.XPATH,'//div[@class=\"grid grid-cols-8 gap-4 md:grid-cols-12\"]/div[1]/p')\n",
    "        data_type.append(d_type.text)\n",
    "    except NoSuchElementException:\n",
    "        data_type.append(None)\n",
    "    \n",
    "    # Associated Tasks \n",
    "    try:\n",
    "        tsk=driver.find_element(By.XPATH,'//div[@class=\"grid grid-cols-8 gap-4 md:grid-cols-12\"]/div[3]/p')\n",
    "        task.append(tsk.text)\n",
    "    except NoSuchElementException:\n",
    "        task.append(None)\n",
    "    \n",
    "    # Attribute Type\n",
    "    try:\n",
    "        attribute=driver.find_element(By.XPATH,'//div[@class=\"grid grid-cols-8 gap-4 md:grid-cols-12\"]/div[4]/p')\n",
    "        attribute_type.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        attribute_type.append(None)\n",
    "    \n",
    "    # No of Instances\n",
    "    try:\n",
    "        instance=driver.find_element(By.XPATH,'//div[@class=\"grid grid-cols-8 gap-4 md:grid-cols-12\"]/div[5]/p')\n",
    "        no_of_instance.append(instance.text)\n",
    "    except NoSuchElementException:\n",
    "        no_of_instance.append(None)\n",
    "    \n",
    "    # Year\n",
    "    try:\n",
    "        yr=driver.find_element(By.XPATH,'//h2[@class=\"text-primary-content\"]')\n",
    "        year.append(yr.text.split(\"/\")[-1])\n",
    "    except NoSuchElementException:\n",
    "        year.append(None)\n",
    "\n",
    "# Quit the Driver\n",
    "driver.quit()\n",
    "                \n",
    "# Creating DataFrame        \n",
    "df_dataset=pd.DataFrame({\"Dataset Name\":name,\n",
    "                         \"Data Type\":data_type,\n",
    "                         \"Associated Tasks\":task,\n",
    "                         \"Attribute Type\":attribute_type,\n",
    "                         \"No of Instances\":no_of_instance,\n",
    "                         \"Year\": year})\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249db237",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company\n",
    "D)Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb98c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of designation=  20\n",
      "length of company=  20\n",
      "length of skills=  20\n",
      "length of locations=  20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L&amp;D Trainer - Python &amp; Data Science/Data Analy...</td>\n",
       "      <td>AVE-Promagne Business Solutions</td>\n",
       "      <td>Data Analytics\\nIT training\\nTraining\\nMachine...</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Bizongo</td>\n",
       "      <td>Vision\\nAnalytics\\nDeep Learning\\nNetworking\\n...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engineer II- Data Science &amp; Analytics</td>\n",
       "      <td>Raytheon Technologies</td>\n",
       "      <td>Data Science\\nstatistical modeling\\nmachine le...</td>\n",
       "      <td>Hybrid - Bangalore/ Bengaluru, Karnataka(Yelah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Augusta Infotech</td>\n",
       "      <td>python\\nnumpy\\nmachine learning\\ntensorflow\\nP...</td>\n",
       "      <td>Bangalore/ Bengaluru, Karnataka(Electronic City)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Analyst, Data Science</td>\n",
       "      <td>DUN BRADSTREET INFORMATION SERVICES INDIA PRIV...</td>\n",
       "      <td>Analysis\\nAnalytical\\nData Science\\nData analy...</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Lead Consultant - Data Science</td>\n",
       "      <td>All About It India</td>\n",
       "      <td>Computer\\nUnix\\nData Science\\nAnalytical\\nData...</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Science Analyst</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Machine learning\\nPredictive modeling\\nData mi...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Science Senior Analyst</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Data Science\\nPredictive Modeling\\nmachine lea...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science - Intern</td>\n",
       "      <td>Zupee</td>\n",
       "      <td>Machine Learning\\nNeural networks\\nDeep Learni...</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Software Engineer, Data Science</td>\n",
       "      <td>Epiq Systems</td>\n",
       "      <td>Data Science\\nSpark\\nScikit-learn\\nKeras\\nSoft...</td>\n",
       "      <td>Hybrid - Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Senior Software Engineer, Data Science</td>\n",
       "      <td>Epiq Systems, Inc.</td>\n",
       "      <td>Languages\\nDevelopment\\nProduct management\\nDa...</td>\n",
       "      <td>Hyderabad/Secunderabad, Canada, Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Senior Analyst, Data Science</td>\n",
       "      <td>Venator Holdings</td>\n",
       "      <td>Recruitment\\nSenior\\nStatistical modeling\\nDat...</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Science, AI/ML Professional</td>\n",
       "      <td>QA InfoTech Pvt. Ltd</td>\n",
       "      <td>Cloud\\nCloud computing\\nData Science\\nSimulati...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Science - AI/ML Professional</td>\n",
       "      <td>QA InfoTech Pvt. Ltd</td>\n",
       "      <td>Ml\\nMySQL\\nSDLC\\nJIRA\\nData Science\\nCloud com...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Conneqt Digital</td>\n",
       "      <td>Data Science\\nAzure\\nArtificial Intelligence\\n...</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Science Analyst</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Data Science\\nArtificial Intelligence\\nData mi...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Science - Manager/ Senior Associate</td>\n",
       "      <td>Merilytics</td>\n",
       "      <td>Decision Tree\\nNatural Language Processing\\nNe...</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Senior Software Engineer, Data Science</td>\n",
       "      <td>Epiq Global</td>\n",
       "      <td>Django\\nNatural language processing\\nUI develo...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Modeller- Data Science/ ML</td>\n",
       "      <td>TRH Consultancy Services</td>\n",
       "      <td>Data Science\\nJava\\nR\\nMachine\\nAlgorithms\\nDa...</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Senior Manager Data Science Global</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Machine Learning\\nMachine\\nPython\\nScience\\nOp...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Designation  \\\n",
       "0   L&D Trainer - Python & Data Science/Data Analy...   \n",
       "1                               Data Science Engineer   \n",
       "2               Engineer II- Data Science & Analytics   \n",
       "3                               Data Science Engineer   \n",
       "4                        Senior Analyst, Data Science   \n",
       "5               Senior Lead Consultant - Data Science   \n",
       "6                                Data Science Analyst   \n",
       "7                         Data Science Senior Analyst   \n",
       "8                               Data Science - Intern   \n",
       "9                     Software Engineer, Data Science   \n",
       "10             Senior Software Engineer, Data Science   \n",
       "11                       Senior Analyst, Data Science   \n",
       "12                   Data Science, AI/ML Professional   \n",
       "13                  Data Science - AI/ML Professional   \n",
       "14                              Data Science Engineer   \n",
       "15                               Data Science Analyst   \n",
       "16           Data Science - Manager/ Senior Associate   \n",
       "17             Senior Software Engineer, Data Science   \n",
       "18                    Data Modeller- Data Science/ ML   \n",
       "19                 Senior Manager Data Science Global   \n",
       "\n",
       "                                              Company  \\\n",
       "0                     AVE-Promagne Business Solutions   \n",
       "1                                             Bizongo   \n",
       "2                               Raytheon Technologies   \n",
       "3                                    Augusta Infotech   \n",
       "4   DUN BRADSTREET INFORMATION SERVICES INDIA PRIV...   \n",
       "5                                  All About It India   \n",
       "6                                           Accenture   \n",
       "7                                           Accenture   \n",
       "8                                               Zupee   \n",
       "9                                        Epiq Systems   \n",
       "10                                 Epiq Systems, Inc.   \n",
       "11                                   Venator Holdings   \n",
       "12                               QA InfoTech Pvt. Ltd   \n",
       "13                               QA InfoTech Pvt. Ltd   \n",
       "14                                    Conneqt Digital   \n",
       "15                                          Accenture   \n",
       "16                                         Merilytics   \n",
       "17                                        Epiq Global   \n",
       "18                           TRH Consultancy Services   \n",
       "19                                            Red Hat   \n",
       "\n",
       "                                               Skills  \\\n",
       "0   Data Analytics\\nIT training\\nTraining\\nMachine...   \n",
       "1   Vision\\nAnalytics\\nDeep Learning\\nNetworking\\n...   \n",
       "2   Data Science\\nstatistical modeling\\nmachine le...   \n",
       "3   python\\nnumpy\\nmachine learning\\ntensorflow\\nP...   \n",
       "4   Analysis\\nAnalytical\\nData Science\\nData analy...   \n",
       "5   Computer\\nUnix\\nData Science\\nAnalytical\\nData...   \n",
       "6   Machine learning\\nPredictive modeling\\nData mi...   \n",
       "7   Data Science\\nPredictive Modeling\\nmachine lea...   \n",
       "8   Machine Learning\\nNeural networks\\nDeep Learni...   \n",
       "9   Data Science\\nSpark\\nScikit-learn\\nKeras\\nSoft...   \n",
       "10  Languages\\nDevelopment\\nProduct management\\nDa...   \n",
       "11  Recruitment\\nSenior\\nStatistical modeling\\nDat...   \n",
       "12  Cloud\\nCloud computing\\nData Science\\nSimulati...   \n",
       "13  Ml\\nMySQL\\nSDLC\\nJIRA\\nData Science\\nCloud com...   \n",
       "14  Data Science\\nAzure\\nArtificial Intelligence\\n...   \n",
       "15  Data Science\\nArtificial Intelligence\\nData mi...   \n",
       "16  Decision Tree\\nNatural Language Processing\\nNe...   \n",
       "17  Django\\nNatural language processing\\nUI develo...   \n",
       "18  Data Science\\nJava\\nR\\nMachine\\nAlgorithms\\nDa...   \n",
       "19  Machine Learning\\nMachine\\nPython\\nScience\\nOp...   \n",
       "\n",
       "                                             Location  \n",
       "0   Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...  \n",
       "1                                 Bangalore/Bengaluru  \n",
       "2   Hybrid - Bangalore/ Bengaluru, Karnataka(Yelah...  \n",
       "3    Bangalore/ Bengaluru, Karnataka(Electronic City)  \n",
       "4                              Hyderabad/Secunderabad  \n",
       "5   Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...  \n",
       "6                                 Bangalore/Bengaluru  \n",
       "7                                              Mumbai  \n",
       "8                                         Delhi / NCR  \n",
       "9                     Hybrid - Hyderabad/Secunderabad  \n",
       "10               Hyderabad/Secunderabad, Canada, Pune  \n",
       "11                             Hyderabad/Secunderabad  \n",
       "12                                Bangalore/Bengaluru  \n",
       "13                                Bangalore/Bengaluru  \n",
       "14                       Hybrid - Bangalore/Bengaluru  \n",
       "15                                             Mumbai  \n",
       "16                             Hyderabad/Secunderabad  \n",
       "17                       Hyderabad/Secunderabad, Pune  \n",
       "18  Mumbai, Hyderabad/Secunderabad, Pune, Chennai,...  \n",
       "19                                Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the driver\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# loading the url page\n",
    "url='https://www.naukri.com/hr-recruiters-consultants'\n",
    "driver.get(url)\n",
    "sleep(5)\n",
    "\n",
    "#click on pop up \n",
    "try:\n",
    "    got_it=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div[2]/div/button')\n",
    "    got_it.click()\n",
    "except NoSuchElementException:\n",
    "    print(\"exception handled\")\n",
    "\n",
    "# click on home page\n",
    "home=driver.find_element(By.XPATH,'/html/body/div[1]/div[3]/div[2]/a/img')\n",
    "home.click()\n",
    "sleep(3)\n",
    "\n",
    "# write on searchbox\n",
    "input_search=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/div[1]/div/input')\n",
    "input_search.send_keys(\"Data Science\")\n",
    "\n",
    "#click on search button\n",
    "search_button=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[6]')\n",
    "search_button.click()\n",
    "sleep(5)\n",
    "\n",
    "# create empty list\n",
    "designation=[]\n",
    "company=[]\n",
    "skills=[]\n",
    "location=[]\n",
    "\n",
    "# designation\n",
    "try:\n",
    "    desig=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "    for i in desig:\n",
    "        designation.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    designation.append(None)\n",
    "\n",
    "# Company Name   \n",
    "try:\n",
    "    comp=driver.find_elements(By.XPATH,'//div[@class=\"companyInfo subheading\"]/a[1]')\n",
    "    for i in comp:\n",
    "        company.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    company.append(None)\n",
    "    \n",
    "\n",
    "# Slills required    \n",
    "try:\n",
    "    skill=driver.find_elements(By.XPATH,'//ul[@class=\"tags has-description\"]')\n",
    "    for i in skill:\n",
    "        skills.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    skills.append(None)\n",
    "\n",
    "# job location    \n",
    "try:\n",
    "    locations=driver.find_elements(By.XPATH,'//li[@class=\"fleft br2 placeHolderLi location\"]/span')\n",
    "    for i in locations:\n",
    "        location.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    location.append(None)\n",
    "\n",
    "print(\"length of designation= \",len(designation))\n",
    "print(\"length of company= \",len(company))\n",
    "print(\"length of skills= \",len(skills))\n",
    "print(\"length of locations= \",len(location))\n",
    "\n",
    "# Quit driver\n",
    "driver.quit()\n",
    "\n",
    "# create dataframe\n",
    "naukri_df=pd.DataFrame({\"Designation\": designation,\n",
    "                        \"Company\": company,\n",
    "                        \"Skills\":skills,\n",
    "                        \"Location\":location})\n",
    "naukri_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
